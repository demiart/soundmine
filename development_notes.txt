Week of May 16 to May 21:
- downloaded and installed MacRuby 0.4
- compared semantic between RubyCocoa and MacRuby to determine which project was more appropriate for this project.
- read and followed the MacRuby tutorial
- watched the MacRuby screencast (1h, 20m) available at PeepCode (https://peepcode.com/products/meet-macruby).  
- XCode not working on my Mac.  It has been on the machine for some time, but will not open.  Working to fix this asap. 
- refreshing knowledge of Cocoa/Objective-C
- posed question on forums to get some assistance regarding the Core Audio classes and usage.
- fixed xcode with software update
- created starter MacRuby project in xcode.
- Read a number of chapters in Aaron Hillegass' "Cocoa Programming for OS X"
- Looked into subclassing NSButton for purposes of making a nice toggle.  Settled on a set default button for now.
- Built basic interface nib with on/off toggle button
- Created starter classes for Objective-C Audio Queue calling code.
- Read answers to questions on core audio.  Refer to:
      1. http://zathras.de/programming/cocoa/UKSoundFileRecorder.zip/
	  2. http://www.cocoadev.com/index.pl?HowToRecordSound
	  3. http://developer.apple.com/documentation/MusicAudio/Conceptual/CoreAudioOverview/Introduction/Introduction.html
- Subclassed and added an NSView for display.  Completed drawRect method (to create background - will become more complex.

Plan for week of May 22 through May 29:
- add in audio listening code when button is activated
- first stage: output to log at intervals some information about data being passed through.
- capture data to file
- if time permits, some in-view display

Week of May 22 to May 29:
- read through CoreAudio documentation (at least a good portion of it - it is massive)
- read more Hillegass
- Found and downloaded a framework, "MTCoreAudio.framework", which is an Objective-C wrapper around the HAL of CoreAudio.  It looks as if it may simplify development.
- Added MTCoreAudio to the project in a dependency directory as a zip file.  In order to build the project the framework must be installed and the reference to the framework corrected.
- MTCoreAudio is located at: http://aldebaran.armory.com/~zenomt/macosx/MTCoreAudio/
- located a number of sample projects using MTCoreAudio, including one that records from the input device.  Ref: http://borkware.com/rants/sound/
- read an article at: http://old.macedition.com/bolts/bolts_20030509.php
- Modified a sample app to determine how to connect to the default Built-in input source using MTCoreAudio.  Was able to note data capture on record.
- Modified target architecture to be able to use MTCoreAudio
- Receiving an error that the app is GC, but the framework (MTCoreAudio) is not.  This needs to be fixed somehow.  Looking into options.
- Opened MTCoreAudio source project, modded, and rebuilt as GC-OK.  Received additional build errors regarding architecture.
- Rebuilt MTCoreAudio framework for correct architecture and OS version.  Linked properly.
- Enum values of MTCoreAudio are not understood within MacRuby... Looking into issue.
- zipped modded framework and placed in dependency directory.
- was able to get the input device linked-in through macruby.
- issues with registering a callback method / selector...
- kept having issues with either the callback registration or with stack dumps - mostly due to unknown selectors.  Looked in forums and found little help.  
- Moved core audio device functionality to objective-c class and kick it off from the ruby - this seems to work well.  Disappointed that the project may be a bit ruby-thin.  I may attempt pulling things back in later to see if i can get them working in the ruby code.
- modified obj-c callback to log a byte in the buffer to prove that the data is variant when recording.  Now need to figure out how to either stream to file or buffer into memory until "stop".
- embedded macruby into the target - the application contents should now include the macruby framework.

Plan for week of May 30 through June 5:
- get the audio file saving and test playback in secondary app.
- visualization as time permits.
- playback from app?

Week of May 30 to June 5:
- Did some research on web and in ADC for how to save audio data to file.
- switched to use audio from built-in mic as it appears to work on a sample app.  I hope to look deeper into how to capture what I want once the rest of the app has been more-or-less completed.
- figured out how to get stream description data and added logging for analytical purposes.
- need to get into the c/c++ level - in preparation for using AudioToolbox and AudioFile libraries, wrote in allocation, population, and dealloc of AudioStreamBasicDescription struct.
- wrote out methods for building structures needed to calling AudioFileCreate
- receiving status error 1718449215 on AudioFileCreate - this is a bad format error.  I am looking into this...
- the format error may have to do with a difference in format between the lpcm input format and the supported formats for aiff, mp3, etc.
- I am uncertain if the format error requires resolution by tweaking bytes, or if I should try out QuickTime.  If this can not be resolved by tomorrow AM, I may segue to QuickTime, which has a higher-level api than AudioToolbox.
- cool. solved format error - wave format will work without additional manipulation.  
- handling of dupFNErr error by erasing contents of previously existing file.
- added code to clear file contents if it already exists.
- added code to push callback buffer data into file, and code to close file on stop.
- tested successfully - records audio from mic to a wav file (listened to file in QuickTime)
- need to add drop down (drawer?) to set file location, remove hardcoded location that is specific to my machine.